{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de673d19",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html\n",
    "\n",
    "ImageNet Dataset Preparation\n",
    "1. Download the following compressed files from https://image-net.org/challenges/LSVRC/2012/2012-downloads.php (require account login).\n",
    "    1. Development kit (Task 1 & 2). 2.5MB.\n",
    "    2. Training images (Task 1 & 2). 138GB.\n",
    "    3. Validation images (all tasks). 6.3GB.\n",
    "    4. Test images (all tasks). 13GB. (optional)\n",
    "2. https://pytorch.org/vision/stable/_modules/torchvision/datasets/imagenet.html#ImageNet\n",
    "\n",
    "Efficient_B7 Pretrained Model\n",
    "1. The pretrained model, efficientnet_b7_lukemelas-dcc49843.pth, is downloaded from https://download.pytorch.org/models/efficientnet_b7_lukemelas-dcc49843.pth.\n",
    "2. The definition of Efficient_B7 can be found at https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py.\n",
    "\n",
    "Problems faced\n",
    "1. \n",
    "```bash\n",
    "AssertionError: did not find fuser method for: (<class 'torch.nn.modules.conv.Conv2d'>, <class 'torch.nn.modules.batchnorm.BatchNorm2d'>, <class 'torch.nn.modules.activation.SiLU'>) \n",
    "```\n",
    "2.\n",
    "```bash\n",
    "NotImplementedError: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [QuantizedCPU, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n",
    "```\n",
    "3.\n",
    "```bash\n",
    "NotImplementedError: Could not run 'aten::empty.memory_format' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty.memory_format' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n",
    "```\n",
    "4.\n",
    "```bash\n",
    "NotImplementedError: Could not run 'aten::empty_strided' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::empty_strided' is only available for these backends: [CPU, CUDA, Meta, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n",
    "```\n",
    "5.\n",
    "```bash\n",
    "NotImplementedError: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, Named, Conjugate, Negative, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradLazy, AutogradXPU, AutogradMLC, AutogradHPU, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, UNKNOWN_TENSOR_TYPE_ID, Autocast, Batched, VmapMode].\n",
    "```\n",
    "6.\n",
    "\n",
    "```bash\n",
    "    213             result = self.stochastic_depth(result)\n",
    "    214             result = self.dequant(result)\n",
    "--> 215             result += input\n",
    "    216             result = self.quant(result)\n",
    "    217         return result\n",
    "RuntimeError: promoteTypes with quantized numbers is not handled yet; figure out what the correct rules should be, offending types: Float QUInt8\n",
    "```\n",
    "Previous 5 problems can be solve by surrounding the operation with QuantStub() and DeQuantStub().\n",
    "And the last problem can be solved with nn.quantized.FloatFunctional()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaef9b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fab5194c948>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import *\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "# # Setup warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    action='ignore',\n",
    "    category=DeprecationWarning,\n",
    "    module=r'.*'\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    action='default',\n",
    "    module=r'torch.quantization'\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb382e",
   "metadata": {},
   "source": [
    "# 1. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a4238c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import math\n",
    "from functools import partial\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "from torchvision.ops import StochasticDepth\n",
    "\n",
    "def _make_divisible(v, divisor, min_value=None):\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    :param v:\n",
    "    :param divisor:\n",
    "    :param min_value:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/ops/misc.py\n",
    "class ConvBNAct(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_planes, \n",
    "                 out_planes, \n",
    "                 kernel_size = 3, \n",
    "                 stride = 1, \n",
    "                 groups = 1, \n",
    "                 norm_layer = nn.BatchNorm2d, \n",
    "                 activation_layer = nn.ReLU, \n",
    "                 dequant = None):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBNAct, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups = groups, bias = False),\n",
    "            norm_layer(out_planes),\n",
    "            # Replace with activation\n",
    "            dequant,\n",
    "            activation_layer(inplace=False),\n",
    "            QuantStub(),\n",
    "        )\n",
    "\n",
    "class ConvBN(nn.Sequential):\n",
    "    def __init__(self, \n",
    "                 in_planes, \n",
    "                 out_planes, \n",
    "                 kernel_size = 3, \n",
    "                 stride = 1, \n",
    "                 groups = 1, \n",
    "                 norm_layer = nn.BatchNorm2d, \n",
    "                 dequant = None):\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        super(ConvBN, self).__init__(\n",
    "            # QuantStub(),\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
    "            norm_layer(out_planes),\\\n",
    "            # dequant,\\\n",
    "        )\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/ops/misc.py\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/quantization/mobilenetv3.py\n",
    "class SqueezeExcitation(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_channels, \n",
    "        squeeze_channels, \n",
    "        activation = torch.nn.ReLU, \n",
    "        scale_activation = torch.nn.Sigmoid, \n",
    "        dequant = None,\n",
    "    ):\n",
    "        super(SqueezeExcitation, self).__init__()\n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = torch.nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.fc2 = torch.nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "        self.quant1 = QuantStub()\n",
    "        self.quant2 = QuantStub()\n",
    "        self.quant3 = QuantStub()\n",
    "        self.dequant = dequant\n",
    "        self.activation = activation()\n",
    "        self.scale_activation = scale_activation()\n",
    "\n",
    "    def _scale(self, input):\n",
    "        scale = self.avgpool(input)\n",
    "        scale = self.fc1(scale)\n",
    "        # https://github.com/pytorch/pytorch/issues/34583\n",
    "        # https://discuss.pytorch.org/t/quantization-how-to-quantize-model-which-include-not-support-to-quantize-layer/76528\n",
    "        # https://pytorch.org/tutorials/recipes/fuse.html\n",
    "        scale = self.dequant(scale)\n",
    "        scale = self.activation(scale)\n",
    "        scale = self.quant1(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        scale = self.dequant(scale)\n",
    "        scale = self.scale_activation(scale)\n",
    "        scale = self.quant2(scale)\n",
    "        return scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        scale = self._scale(input)\n",
    "        x = self.dequant(scale) * self.dequant(input)\n",
    "        x = self.quant2(x)\n",
    "        return x\n",
    "\n",
    "# https://github.com/pytorch/vision/blob/main/torchvision/models/efficientnet.py\n",
    "class MBConvConfig:\n",
    "    # Stores information listed at Table 1 of the EfficientNet paper\n",
    "    def __init__(\n",
    "        self,\n",
    "        expand_ratio: float,\n",
    "        kernel: int,\n",
    "        stride: int,\n",
    "        input_channels: int,\n",
    "        out_channels: int,\n",
    "        num_layers: int,\n",
    "        width_mult: float,\n",
    "        depth_mult: float,\n",
    "    ) -> None:\n",
    "        self.expand_ratio = expand_ratio\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        self.num_layers = self.adjust_depth(num_layers, depth_mult)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        s = self.__class__.__name__ + \"(\"\n",
    "        s += \"expand_ratio={expand_ratio}\"\n",
    "        s += \", kernel={kernel}\"\n",
    "        s += \", stride={stride}\"\n",
    "        s += \", input_channels={input_channels}\"\n",
    "        s += \", out_channels={out_channels}\"\n",
    "        s += \", num_layers={num_layers}\"\n",
    "        s += \")\"\n",
    "        return s.format(**self.__dict__)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_mult: float) -> int:\n",
    "        return _make_divisible(channels * width_mult, 8)\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_depth(num_layers: int, depth_mult: float):\n",
    "        return int(math.ceil(num_layers * depth_mult))\n",
    "\n",
    "# https://paperswithcode.com/method/inverted-residual-block\n",
    "# https://towardsdatascience.com/mobilenetv2-inverted-residuals-and-linear-bottlenecks-8a4362f4ffd5\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cnf: MBConvConfig,\n",
    "        stochastic_depth_prob: float,\n",
    "        norm_layer,\n",
    "        se_layer = SqueezeExcitation,\n",
    "        dequant = None,\n",
    "    ) -> None:\n",
    "        super(MBConv, self).__init__()\n",
    "\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError(\"illegal stride value\")\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers = []\n",
    "        # activation_layer = nn.ReLU\n",
    "        activation_layer = nn.SiLU\n",
    "\n",
    "        # expand\n",
    "        expanded_channels = cnf.adjust_channels(cnf.input_channels, cnf.expand_ratio)\n",
    "        if expanded_channels != cnf.input_channels:\n",
    "            layers.append(\n",
    "                ConvBNAct(cnf.input_channels, \n",
    "                          expanded_channels, \n",
    "                          kernel_size = 1, \n",
    "                          norm_layer = norm_layer, \n",
    "                          activation_layer = activation_layer, \n",
    "                          dequant = dequant)\n",
    "            )\n",
    "\n",
    "        # depthwise\n",
    "        layers.append(\n",
    "            ConvBNAct(\n",
    "                expanded_channels,\n",
    "                expanded_channels,\n",
    "                kernel_size = cnf.kernel,\n",
    "                stride = cnf.stride,\n",
    "                groups = expanded_channels,\n",
    "                norm_layer = norm_layer,\n",
    "                activation_layer = activation_layer,\n",
    "                dequant = dequant,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # squeeze and excitation\n",
    "        squeeze_channels = max(1, cnf.input_channels // 4)\n",
    "        layers.append(se_layer(expanded_channels, squeeze_channels, activation=partial(activation_layer, inplace=True), dequant=dequant))\n",
    "\n",
    "        # project\n",
    "        layers.append(\n",
    "            ConvBN(\n",
    "                expanded_channels, cnf.out_channels, kernel_size = 1, norm_layer = norm_layer, dequant = dequant,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, \"row\")\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = dequant\n",
    "        self.out_channels = cnf.out_channels\n",
    "        self.skip_add = nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, input):\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result = self.stochastic_depth(result)\n",
    "            result = self.skip_add.add(result, input)\n",
    "        return result\n",
    "\n",
    "\n",
    "class EfficientNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inverted_residual_setting,\n",
    "        dropout: float,\n",
    "        stochastic_depth_prob: float = 0.2,\n",
    "        num_classes: int = 1000,\n",
    "        norm_layer = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        EfficientNet main class\n",
    "        Args:\n",
    "            inverted_residual_setting (List[MBConvConfig]): Network structure\n",
    "            dropout (float): The droupout probability\n",
    "            stochastic_depth_prob (float): The stochastic depth probability\n",
    "            num_classes (int): Number of classes\n",
    "        \"\"\"\n",
    "        super(EfficientNet, self).__init__()\n",
    "\n",
    "        block = MBConv\n",
    "        # activation_layer = nn.ReLU\n",
    "        activation_layer = nn.SiLU\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        layers = []\n",
    "        \n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "\n",
    "        # building first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(\n",
    "            ConvBNAct(\n",
    "                3, \n",
    "                firstconv_output_channels, \n",
    "                kernel_size = 3, \n",
    "                stride = 2, \n",
    "                norm_layer = norm_layer, \n",
    "                activation_layer = activation_layer,\n",
    "                dequant = self.dequant,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        total_stage_blocks = sum(cnf.num_layers for cnf in inverted_residual_setting)\n",
    "        stage_block_id = 0\n",
    "        for cnf in inverted_residual_setting:\n",
    "            stage: List[nn.Module] = []\n",
    "            for _ in range(cnf.num_layers):\n",
    "                # copy to avoid modifications. shallow copy is enough\n",
    "                block_cnf = copy.copy(cnf)\n",
    "\n",
    "                # overwrite info if not the first conv in the stage\n",
    "                if stage:\n",
    "                    block_cnf.input_channels = block_cnf.out_channels\n",
    "                    block_cnf.stride = 1\n",
    "\n",
    "                # adjust stochastic depth probability based on the depth of the stage block\n",
    "                sd_prob = stochastic_depth_prob * float(stage_block_id) / total_stage_blocks\n",
    "\n",
    "                stage.append(block(block_cnf, sd_prob, norm_layer, dequant = self.dequant))\n",
    "                stage_block_id += 1\n",
    "\n",
    "            layers.append(nn.Sequential(*stage))\n",
    "\n",
    "        # building last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = 4 * lastconv_input_channels\n",
    "        layers.append(\n",
    "            ConvBNAct(\n",
    "                lastconv_input_channels,\n",
    "                lastconv_output_channels,\n",
    "                kernel_size = 1,\n",
    "                norm_layer = norm_layer,\n",
    "                activation_layer = activation_layer,\n",
    "                dequant = self.dequant,\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=dropout, inplace=True),\n",
    "            nn.Linear(lastconv_output_channels, num_classes),\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init_range = 1.0 / math.sqrt(m.out_features)\n",
    "                nn.init.uniform_(m.weight, -init_range, init_range)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self._forward_impl(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "    \n",
    "    # Fuse Conv+BN and Conv+BN+Relu modules prior to quantization\n",
    "    # This operation does not change the numerics\n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == ConvBNAct:\n",
    "                # https://github.com/pytorch/pytorch/issues/41534\n",
    "                # https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106\n",
    "                # torch.quantization.fuse_modules(m, ['0', '1', '2'], inplace=True)\n",
    "                torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)\n",
    "            if type(m) == ConvBN:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)\n",
    "            if type(m) == MBConv:\n",
    "                for idx in range(len(m.block)):\n",
    "                    if type(m.block[idx]) == nn.Conv2d:\n",
    "                        torch.quantization.fuse_modules(m.block, [str(idx), str(idx + 1)], inplace=True)\n",
    "\n",
    "\n",
    "def _efficientnet(\n",
    "    arch: str,\n",
    "    width_mult: float,\n",
    "    depth_mult: float,\n",
    "    dropout: float,\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    norm_layer,\n",
    ") -> EfficientNet:\n",
    "    bneck_conf = partial(MBConvConfig, width_mult=width_mult, depth_mult=depth_mult)\n",
    "    inverted_residual_setting = [\n",
    "        bneck_conf(1, 3, 1, 32, 16, 1),\n",
    "        bneck_conf(6, 3, 2, 16, 24, 2),\n",
    "        bneck_conf(6, 5, 2, 24, 40, 2),\n",
    "        bneck_conf(6, 3, 2, 40, 80, 3),\n",
    "        bneck_conf(6, 5, 1, 80, 112, 3),\n",
    "        bneck_conf(6, 5, 2, 112, 192, 4),\n",
    "        bneck_conf(6, 3, 1, 192, 320, 1),\n",
    "    ]\n",
    "    model = EfficientNet(inverted_residual_setting, dropout, norm_layer=norm_layer)\n",
    "    if pretrained:\n",
    "        if model_urls.get(arch, None) is None:\n",
    "            raise ValueError(f\"No checkpoint is available for model type {arch}\")\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def efficientnet_b7(pretrained: bool = False, progress: bool = True) -> EfficientNet:\n",
    "    \"\"\"\n",
    "    Constructs a EfficientNet B7 architecture from\n",
    "    `\"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\" <https://arxiv.org/abs/1905.11946>`_.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _efficientnet(\n",
    "        \"efficientnet_b7\",\n",
    "        2.0,\n",
    "        3.1,\n",
    "        0.5,\n",
    "        pretrained,\n",
    "        progress,\n",
    "        norm_layer=partial(nn.BatchNorm2d, eps=0.001, momentum=0.01),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc604eb",
   "metadata": {},
   "source": [
    "# 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41b6f9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches, device='cpu'):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            image, target = image.to(device), target.to(device)\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "def load_model(model_file):\n",
    "    model = efficientnet_b7(pretrained=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af7b6eb",
   "metadata": {},
   "source": [
    "# 3. Define Dataset and Data Loaders\n",
    "## ImageNet Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7918d2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_loaders(data_path):\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    dataset = torchvision.datasets.ImageNet(\n",
    "           data_path, split=\"train\", transform=transforms.Compose([\n",
    "               transforms.RandomResizedCrop(224),\n",
    "               transforms.RandomHorizontalFlip(),\n",
    "               transforms.ToTensor(),\n",
    "               normalize,\n",
    "           ]))\n",
    "    dataset_test = torchvision.datasets.ImageNet(\n",
    "          data_path, split=\"val\", transform=transforms.Compose([\n",
    "              transforms.Resize(256),\n",
    "              transforms.CenterCrop(224),\n",
    "              transforms.ToTensor(),\n",
    "              normalize,\n",
    "          ]))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0113117",
   "metadata": {},
   "source": [
    "## Pre-Trained Efficient_B7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "103ebf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Inverted Residual Block: Before fusion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNAct(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False)\n",
      "    (1): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "    (2): DeQuantStub()\n",
      "    (3): SiLU()\n",
      "    (4): QuantStub()\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (quant1): QuantStub()\n",
      "    (quant2): QuantStub()\n",
      "    (quant3): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "    (activation): SiLU(inplace=True)\n",
      "    (scale_activation): Sigmoid()\n",
      "  )\n",
      "  (2): ConvBN(\n",
      "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "  )\n",
      ")\n",
      "\n",
      " Inverted Residual Block: After fusion\n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNAct(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
      "    (1): Identity()\n",
      "    (2): DeQuantStub()\n",
      "    (3): SiLU()\n",
      "    (4): QuantStub()\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fc2): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (quant1): QuantStub()\n",
      "    (quant2): QuantStub()\n",
      "    (quant3): QuantStub()\n",
      "    (dequant): DeQuantStub()\n",
      "    (activation): SiLU(inplace=True)\n",
      "    (scale_activation): Sigmoid()\n",
      "  )\n",
      "  (2): ConvBN(\n",
      "    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_batch_size = 30\n",
    "eval_batch_size = 50\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "float_model = load_model(saved_model_dir + efficient_float_model_file).to(device)\n",
    "\n",
    "# Next, we'll \"fuse modules\"; this can both make the model faster by saving on memory access\n",
    "# while also improving numerical accuracy. While this can be used with any model, this is\n",
    "# especially common with quantized models.\n",
    "\n",
    "print('\\n Inverted Residual Block: Before fusion \\n\\n', float_model.features[1][0].block)\n",
    "float_model.eval()\n",
    "\n",
    "# Fuses modules\n",
    "float_model.fuse_model()\n",
    "\n",
    "# Note fusion of Conv+BN+Relu and Conv+Relu\n",
    "print('\\n Inverted Residual Block: After fusion\\n\\n', float_model.features[1][0].block)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d310a2c",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3b4fe96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 265.030073\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 73.93\n",
      "CPU times: user 1h 12min 39s, sys: 20.2 s, total: 1h 12min 59s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_eval_batches = 1000\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches, device=device)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + efficient_scripted_float_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cecb7766",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernie55ernie/.local/lib/python3.6/site-packages/torch/ao/quantization/observer.py:174: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization Prepare: Inserting Observers\n",
      "\n",
      " Inverted Residual Block:After observer insertion \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNAct(\n",
      "    (0): Conv2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): DeQuantStub()\n",
      "    (3): SiLU()\n",
      "    (4): QuantStub(\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(\n",
      "      64, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (fc2): Conv2d(\n",
      "      16, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (quant1): QuantStub(\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (quant2): QuantStub(\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (quant3): QuantStub(\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (dequant): DeQuantStub()\n",
      "    (activation): SiLU(inplace=True)\n",
      "    (scale_activation): Sigmoid()\n",
      "  )\n",
      "  (2): ConvBN(\n",
      "    (0): Conv2d(\n",
      "      64, 32, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
      "    )\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "................................Post Training Quantization: Calibration done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernie55ernie/.local/lib/python3.6/site-packages/torch/ao/quantization/utils.py:158: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  \"Returning default values.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " Inverted Residual Block: After fusion and quantization, note fused modules: \n",
      "\n",
      " Sequential(\n",
      "  (0): ConvBNAct(\n",
      "    (0): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.6064728498458862, zero_point=71, padding=(1, 1), groups=64)\n",
      "    (1): Identity()\n",
      "    (2): DeQuantize()\n",
      "    (3): SiLU()\n",
      "    (4): Quantize(scale=tensor([0.2677]), zero_point=tensor([1]), dtype=torch.quint8)\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): QuantizedConv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), scale=0.11322791874408722, zero_point=41)\n",
      "    (fc2): QuantizedConv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), scale=0.07366853207349777, zero_point=51)\n",
      "    (quant1): Quantize(scale=tensor([0.0793]), zero_point=tensor([4]), dtype=torch.quint8)\n",
      "    (quant2): Quantize(scale=tensor([0.2489]), zero_point=tensor([1]), dtype=torch.quint8)\n",
      "    (quant3): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "    (dequant): DeQuantize()\n",
      "    (activation): SiLU(inplace=True)\n",
      "    (scale_activation): Sigmoid()\n",
      "  )\n",
      "  (2): ConvBN(\n",
      "    (0): QuantizedConv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), scale=0.6339235305786133, zero_point=62)\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 67.537169\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 0.40\n",
      "CPU times: user 12h 27min 42s, sys: 6h 35min 16s, total: 19h 2min 58s\n",
      "Wall time: 57min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_calibration_batches = 32\n",
    "\n",
    "myModel = load_model(saved_model_dir + efficient_float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "myModel.fuse_model()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1][0].block)\n",
    "\n",
    "# Calibrate with the training set\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n', myModel.features[1][0].block)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bdcc288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=True){}, weight=functools.partial(<class 'torch.ao.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric){})\n",
      "................................"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ernie55ernie/.local/lib/python3.6/site-packages/torch/ao/quantization/observer.py:886: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_begin // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/ernie55ernie/.local/lib/python3.6/site-packages/torch/ao/quantization/observer.py:891: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  src_bin_end // dst_bin_width, 0, self.dst_nbins - 1\n",
      "/home/ernie55ernie/.local/lib/python3.6/site-packages/torch/ao/quantization/observer.py:1109: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  Returning default scale and zero point \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................Evaluation accuracy on 50000 images, 68.13\n",
      "CPU times: user 12h 50min 44s, sys: 6h 14min 25s, total: 19h 5min 9s\n",
      "Wall time: 57min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + efficient_float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "per_channel_quantized_model.fuse_model()\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True)\n",
    "evaluate(per_channel_quantized_model, criterion, data_loader, num_calibration_batches)\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True)\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + efficient_scripted_quantized_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2f9238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, criterion, optimizer, data_loader, device, ntrain_batches):\n",
    "    model.train()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    avgloss = AverageMeter('Loss', '1.5f')\n",
    "\n",
    "    cnt = 0\n",
    "    for image, target in data_loader:\n",
    "        start_time = time.time()\n",
    "        print('.', end = '')\n",
    "        cnt += 1\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "        top1.update(acc1[0], image.size(0))\n",
    "        top5.update(acc5[0], image.size(0))\n",
    "        avgloss.update(loss, image.size(0))\n",
    "        if cnt >= ntrain_batches:\n",
    "            print('Loss', avgloss.avg)\n",
    "\n",
    "            print('Training: * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}'\n",
    "                  .format(top1=top1, top5=top5))\n",
    "            return\n",
    "\n",
    "    print('Full imagenet train set:  * Acc@1 {top1.global_avg:.3f} Acc@5 {top5.global_avg:.3f}'\n",
    "          .format(top1=top1, top5=top5))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11bf5623",
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = load_model(saved_model_dir + efficient_float_model_file)\n",
    "qat_model.fuse_model()\n",
    "\n",
    "optimizer = torch.optim.SGD(qat_model.parameters(), lr = 0.0001)\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "990df9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverted Residual Block: After preparation for QAT, note fake-quantization modules \n",
      " Sequential(\n",
      "  (0): ConvBNAct(\n",
      "    (0): ConvBn2d(\n",
      "      64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64, bias=False\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "    (2): DeQuantStub()\n",
      "    (3): SiLU()\n",
      "    (4): QuantStub(\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): SqueezeExcitation(\n",
      "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "    (fc1): Conv2d(\n",
      "      64, 16, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (fc2): Conv2d(\n",
      "      16, 64, kernel_size=(1, 1), stride=(1, 1)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (quant1): QuantStub(\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (quant2): QuantStub(\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (quant3): QuantStub(\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (dequant): DeQuantStub()\n",
      "    (activation): SiLU(inplace=True)\n",
      "    (scale_activation): Sigmoid(\n",
      "      (activation_post_process): FixedQParamsFakeQuantize(fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), scale=tensor([0.0039]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=255, qscheme=torch.per_tensor_affine)\n",
      "    )\n",
      "  )\n",
      "  (2): ConvBN(\n",
      "    (0): ConvBn2d(\n",
      "      64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
      "      (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
      "        (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
      "      )\n",
      "      (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
      "        fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
      "        (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
      "      )\n",
      "    )\n",
      "    (1): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "print('Inverted Residual Block: After preparation for QAT, note fake-quantization modules \\n', qat_model.features[1][0].block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "217d259d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................Loss tensor(4.0579, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 32.000 Acc@5 53.167\n",
      "....................Loss tensor(2.8443, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 45.000 Acc@5 69.667\n",
      "....................Loss tensor(2.5352, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 52.167 Acc@5 74.000\n",
      "....................Loss tensor(2.6714, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 49.167 Acc@5 70.833\n",
      "....................Loss tensor(1.8900, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 60.167 Acc@5 82.000\n",
      "....................Loss tensor(1.6780, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 64.833 Acc@5 85.667\n",
      "....................Loss tensor(1.7182, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 64.333 Acc@5 85.000\n",
      "....................Loss tensor(1.6748, grad_fn=<DivBackward0>)\n",
      "Training: * Acc@1 66.333 Acc@5 85.000\n",
      "........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................After training :Evaluation accuracy on 50000 images, 67.17\n",
      "CPU times: user 22h 21min 29s, sys: 7h 26min 51s, total: 1d 5h 48min 20s\n",
      "Wall time: 1h 27min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "num_train_batches = 20\n",
    "\n",
    "# QAT takes time and one needs to train over a few epochs.\n",
    "# Train and check accuracy after each epoch\n",
    "for nepoch in range(8):\n",
    "    train_one_epoch(qat_model, criterion, optimizer, data_loader, torch.device('cpu'), num_train_batches)\n",
    "    if nepoch > 3:\n",
    "        # Freeze quantizer parameters\n",
    "        qat_model.apply(torch.quantization.disable_observer)\n",
    "    if nepoch > 2:\n",
    "        # Freeze batch norm mean and variance estimates\n",
    "        qat_model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n",
    "\n",
    "# Check the accuracy after training\n",
    "quantized_model = torch.quantization.convert(qat_model.eval(), inplace=False)\n",
    "quantized_model.eval()\n",
    "top1, top5 = evaluate(quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('After training :Evaluation accuracy on %d images, %2.2f' % (num_eval_batches * eval_batch_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccd8edff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time:  78 ms\n",
      "Elapsed time: 374 ms\n",
      "CPU times: user 9min 5s, sys: 4min 10s, total: 13min 15s\n",
      "Wall time: 2min 5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "93.48641204833984"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file).to('cpu')\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "run_benchmark(saved_model_dir + efficient_scripted_float_model_file, data_loader_test)\n",
    "\n",
    "run_benchmark(saved_model_dir + efficient_scripted_quantized_model_file, data_loader_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc1b887",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [PyTorch feature classification changes](https://pytorch.org/blog/pytorch-feature-classification-changes/)\n",
    "2. [QUANTIZATION](https://pytorch.org/docs/stable/quantization.html)\n",
    "3. [TRAINING A CLASSIFIER](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#where-do-i-go-next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169a2801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
